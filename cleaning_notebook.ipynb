{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install sematch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import gc\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import spacy\n",
    "import textacy\n",
    "import en_core_web_md\n",
    "import sematch\n",
    "import gensim\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from scipy import sparse\n",
    "from scipy.optimize import minimize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import textacy\n",
    "import en_core_web_md\n",
    "import sematch\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "# defining the cleaning functions\n",
    "def text_to_wordlist(text):\n",
    "\n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    return(text)\n",
    "\n",
    "def concat_words(df2):\n",
    "    df = df2.copy()\n",
    "    df['Description'] = df['Description'].apply(lambda x: (' '.join(i for i in x)))\n",
    "    return df\n",
    "\n",
    "def basic_cleaning2(string):\n",
    "    string = str(string)\n",
    "    string = re.sub('[0-9\\(\\)\\!\\^\\%\\$\\'\\\"\\.;,-\\?\\{\\}\\[\\]\\\\/]', ' ', string)\n",
    "    string = re.sub(' +', ' ', string)\n",
    "    return string\n",
    "\n",
    "# simple cleaning 1\n",
    "def clean_part1(df2):\n",
    "    df = df2.copy()\n",
    "    df['Description'] = df['Description'].apply(lambda x: text_to_wordlist(x))\n",
    "    return df\n",
    "\n",
    "# simple cleaning 2\n",
    "def clean_part2(df2):\n",
    "    df = df2.copy()\n",
    "    df['Description'] = df['Description'].apply(lambda x: basic_cleaning2(x).split())\n",
    "    df['Description'] = df['Description'].apply(lambda x: (' '.join(i for i in x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    return - WORDS[word] \n",
    "\n",
    "def correction(word): \n",
    "    try:\n",
    "        corrected = max(candidates(word), key=P)\n",
    "        return corrected\n",
    "    except Exception:\n",
    "        return word\n",
    "\n",
    "def candidates(word): \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "# replaces mis spelled words\n",
    "def known(words): \n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "# simple edits\n",
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tokenizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main cleaning function\n",
    "\n",
    "def clean_description():\n",
    "    t = time.time()\n",
    "    \n",
    "    # reading train and test sets\n",
    "    df_train = pd.read_csv('train.csv')\n",
    "    df_test = pd.read_csv('test.csv')\n",
    "    \n",
    "    # setting nan in test and concatinating both\n",
    "    df_test['Is_Response'] = np.nan\n",
    "    df = pd.concat([df_train, df_test]).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # perform basic cleanings\n",
    "    df = clean_part1(df)\n",
    "    df = clean_part2(df)\n",
    "    \n",
    "    # clean using textacy\n",
    "    print('Cleaning using textacy.')\n",
    "    df['Description'] = df['Description'].apply(lambda x: textacy.preprocess.preprocess_text(x, fix_unicode = True,\n",
    "                                                                            lowercase = True,\n",
    "                                                                            no_contractions = True,\n",
    "                                                                            transliterate = True\n",
    "                                                                            ))\n",
    "    # simple leammtizations..\n",
    "    print('Lemmatizing text.')\n",
    "    SYMBOLS = set(' '.join(string.punctuation).split(' ') + ['...', '“', '”', '\\'ve'])\n",
    "    q1 = []\n",
    "    for doc in tqdm(nlp.pipe(df['Description'], n_threads=2, batch_size=10000)):\n",
    "        word_list = ([c.lemma_ for c in doc if c.lemma_ not in SYMBOLS])\n",
    "        q1.append(' '.join(i for i in word_list))\n",
    "\n",
    "    q1 = pd.DataFrame(q1)\n",
    "\n",
    "    df['Description'] = q1\n",
    "\n",
    "    # simple word corrections..\n",
    "    print('Correcting words. Using most probable substitutes.')\n",
    "    df['Description'] = df['Description'].apply(lambda x: (' '.join([correction(i) for i in word_tokenize(x)])))\n",
    "\n",
    "\n",
    "    print('Text cleaning done, time it took:', time.time() - t)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download web_md data\n",
    "\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call function to clean the description\n",
    "df = clean_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the data frame\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to csv\n",
    "df.to_csv('alldata_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
